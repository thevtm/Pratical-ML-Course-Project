---
title: "Pratical Machine Learning Project"
author: "Vin√≠cius T. Manjabosco"
date: "24-01-2015"
output: html_document
---

We use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants and our goal is to predict the manner in which the participants did barbell lifts, correctly and incorrectly in 5 different ways, denoted by the "classe" variable in the training set.

## Loading Data

```{r, cache=TRUE}
# Load data
training <- read.csv('pml-training.csv', header = T)
testing <- read.csv('pml-testing.csv', header = T)
```

## Cleaning Data

```{r, cache=TRUE}
library(dplyr, warn.conflicts = F)

colTBU <- intersect(names(training), names(testing))

# Remove identifier columns
identColumns <- c('X', 'user_name')
colTBU <- setdiff(colTBU, identColumns)

# Remove Time related columns
timeColumns <- c('raw_timestamp_part_1', 'raw_timestamp_part_2', 'cvtd_timestamp')
colTBU <- setdiff(colTBU, timeColumns)

# Remove Na Columns
naColumnsTrain <- sapply(training, function(x) any(is.na(x)))
naColumnsTrain <- names(naColumnsTrain[naColumnsTrain])

naColumnsTest <- sapply(testing, function(x) any(is.na(x)))
naColumnsTest <- names(naColumnsTest[naColumnsTest])

naColumns <- union(naColumnsTrain, naColumnsTest)
colTBU <- setdiff(colTBU, naColumns)

# Remove Columns
training <- training[, c(colTBU, 'classe')]
```

We removed the following columns:
* Columns containing identification
* Columns containing timestamp
* Columns containing NAs

```{r}
# Convert factor columns to numeric
for(n in colTBU) {
  if(is.factor(training[, n]) &&
     !(n %in% c('classe', 'new_window'))) {
    training[, n] <- as.numeric(training[, n])
    testing[, n] <- as.numeric(testing[, n])
  }
}
```

We also converted some factors columns to numeric.

## Training

In order to do the training we choose the Random Forest method which is known to make highly accurate predictions. We used a sample of 5000 observations and set its number of trees to 100 and define a 5-fold cross-validation method in the train control.

```{r, cache=TRUE}
library(caret, warn.conflicts = F)
set.seed(123)

sampleSize <- 5000
samples <- sample(nrow(training), sampleSize)
nTrees <- 100

fit <- train(classe ~ ., data = training[samples,],
             method = 'rf', prox = T, ntree = nTrees,
             trControl = trainControl('cv', number = 5))
fit
```

The selected model has mtry = 28 and an accuracy of 98.5% as it can be seen in the following plot

```{r, cache=TRUE}
plot(fit)
```

```{r, cache=TRUE}
fit$finalModel
```

The Out of Sample(OOB) rate is pretty low 1.3%, we should expect a larger error rate in the test set.

## Prediction

Finally, we use our trained model in order to predict the outcomes for the testing set. Considering our OOB error rate, we should expect at least 1 misclassification every 76 cases.

```{r, cache=TRUE}
predict(fit, testing)
```

Our testing set has just 20 cases and, as it turns out, our model was able to predict correctly each and every one of them.